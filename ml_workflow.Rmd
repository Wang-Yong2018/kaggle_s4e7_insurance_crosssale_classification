---
title: "ml_workflow"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Target
It is a kaggle competition url : https://www.kaggle.com/competitions/playground-series-s4e7

The objective of this competition is to predict which customers respond positively to an automobile insurance offer.

ML tools: 
  tidymodels related worksflows & glm, lightgbm, ranger engine.

Evaluatioin metric:
  roc_auc

## librar y & load_data

### library

```{r}
library(tidyverse)
library(tidymodels)
library(future)
library(purrr)
library(furrr)
library(textrecipes)
library(themis)

library(bonsai)
library(lightgbm)
library(xgboost)
library(ranger)

library(readr)
library(janitor)
library(lubridate)
```

```{r}
#TODO, column 18,7 has some strange data leave it handle future. 71 lines in train, 159 line in test
# row833049 column 18 is f has-ring is total wrong. 
# so keep using readr:read_csv handling
```

### loading data

```{r}
data_path <- '../input/playground-series-s4e7/'
n_max=10**5
train<- 
  readr::read_csv(file.path(data_path, 'train.csv'),
                  show_col_types = FALSE,
                  n_max=n_max)|>
  janitor::clean_names()|>
  mutate(response=as.factor(response))
test <- 
  readr::read_csv(file.path(data_path, 'test.csv'),
                  show_col_types = FALSE,
                  n_max=n_max)|>
  janitor::clean_names()
submission <-  readr::read_csv(file.path(data_path, 'sample_submission.csv'),
                  show_col_types = FALSE,
                  n_max=n_max)
```

### quick skim

```{r}
#train|> skimr::skim()
```

```{r}
#test|> skimr::skim()
```

```{r}
#submission |> skimr::skim()
```

### check if train & test is same distribution

```{r}
# get_df_var<-function(df){
#   df|>
#     summarize_all(var)|>
#     pivot_longer(cols=everything(),
#                  names_to='feature',
#                  values_to='variance')
#   
# }
# train|>get_df_var()
```

## coding

### 1. Data Loading and Initial Exploration ----

### 2. Feature Engineering ----

-   leave it in the preprocessing recipe

### 3. Data Splitting ----

#### augment_df

#### split/cv

```{r}
set.seed(1234)

df_split <- initial_split(train, prop = 0.8, strata = response)
 train_set <- training(df_split)
 test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,v = 5,strata=response)
```

### 4. Preprocessing Recipe ----

#### 4.1 v0 base_line 
```{r}
rcp_bs_v0 <-
  recipe(response ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  # hangle categorical / nominal features.
  step_mutate(driving_license=as.factor(driving_license),
              region_code=as.factor(region_code),
              previously_insured=as.factor(previously_insured),
              policy_sales_channel=as.factor(policy_sales_channel)) |>
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(),threshold = 0.01)|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  # handle numeric features
  step_impute_median(all_numeric_predictors())|> 
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  # process the logical feature to factor feature
  # proceeding the convert the character feature to factorical feature
  step_zv(all_predictors())|>
  step_corr(all_numeric_predictors(),threshold = 0.99)|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
 # step_pca(all_numeric_predictors())|>
  step_smote(response, over_ratio = 1.0) |>
  check_missing(all_predictors())

```


#### 4.10 all recipes

```{r}
set.seed(1234)
library(future)
library(furrr)
selected_rcps <- list(base=rcp_bs_v0#,
                      #v1=rcp_bs_v1
                      )
plan(multisession,workers = 5)
#selected_rcps|>map(\(rcp_item) rcp_item|>prep()|>bake(new_data=train)|>summary())
plan(sequential)
```

### 5. Model Specification ----

```{r}
glm_eng <- 
  logistic_reg(penalty = 0.01,
               mixture = 0.5) |>  # Example penalty and mixture values
  set_engine("glmnet") |>
  set_mode("classification")    # Specify classification

lgbm_eng<-
   boost_tree(
      trees = 800, # Number of trees
      learn_rate = 0.1,
      loss_reduction = 0.001,
      sample_size = 0.85, # Added sample_size
      tree_depth = 11,
      #mtry = tune(),
      min_n = 30
   ) |>
   set_mode("classification")|>
   set_engine("lightgbm",
              metric='roc_auc', 
               num_leaves = 900,
              num_threads = 4,
              verbose=1) 

rf_eng<- rand_forest( trees = 500, 
                      #mtry=tune(), 
                      min_n=30) |>
  set_engine("ranger",num.threads=4)|>
  set_mode("classification") 

xgb_eng<- boost_tree( trees = 500, 
                      #mtry=tune(), 
                      min_n=30) |>
  set_engine("xgboost",num.threads=8)|>
  set_mode("classification") 
selected_eng <- list(glm=glm_eng,
                     rf=rf_eng,
                     lgbm=lgbm_eng,
                     xgb=xgb_eng)

```

### 6. Workflow ----
#### set metrics
```{r}
rocauc_metrics <- metric_set(accuracy, bal_accuracy,roc_auc)
```

#### simple wflow

```{r}
# set.seed(1234)
# simple_wf_fit <-
#   workflow() |>
#   add_recipe(rcp_bs_v0) |>
#   add_model(lgbm_eng)|>
#   last_fit(df_split,
#            metrics=rocauc_metrics)
# 
# simple_wf_fit|>collect_metrics()
# simple_wf_fit |>
#   extract_fit_engine()|>
#   plot()

```

#### simple workflowset

```{r}
# set.seed(1234)
# library(future)
# plan(multisession,workers = 12)
# ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE)
# wfs_result <-
#   workflow_set(preproc = selected_rcps,
#                models = selected_eng ) |>
#   workflow_map(fn='fit_resamples',
#                resamples =cv_folds,
#                metrics =rocauc_metrics,
#                control = ctrl
#                )
# wfs_result|>collect_metrics()  
#   
# plan(sequential)
```

### 7 stacking

```{r}
# set.seed(1234)
# library(future)
# plan(multisession,workers = 12)
# combined_fit <-
#   stacks::stacks()|>
#   stacks::add_candidates(wfs_result)|>
#   stacks::blend_predictions()|>
#   stacks::fit_members()
# 
# combined_fit|>
#   autoplot(type = "weights")
# 
# autoplot(combined_fit)
# plan(sequential)
```

### 7. Tuning Grid ----

```{r}
library(finetune)
tune_glm_eng <- 
  logistic_reg(penalty = tune(),
               mixture = tune()) |>  # Example penalty and mixture values
  set_engine("glmnet") |>
  set_mode("classification") 

tune_lgb_eng<-
   boost_tree(
      trees = tune(), # Number of trees
      learn_rate = 0.05,
      loss_reduction = 0.001,
      sample_size = 0.85, # Added sample_size
      tree_depth = tune(),
      #mtry = tune(),
      min_n = tune()
   ) |>
   set_mode("classification")|>
   set_engine("lightgbm",
              metric='roc_auc', 
              num_leaves = tune(),
              #num_threads = 4,
              verbose=1) 

get_tune_grid <- function(tune_eng, size=20){
  tune_eng |>
    extract_parameter_set_dials()|>
    #update(penalty=penalty(range=c(-1,2)))|>
    grid_space_filling(size={size})
}

tune_glm_grid = get_tune_grid(tune_glm_eng,size=10)
tune_lgb_grid = get_tune_grid(tune_lgb_eng,size=10)


set.seed(1234)
library(future)
plan(multisession,workers = 4)
ctrl_resample  <- control_resamples(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE)
ctrl_race <- control_race(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE,verbose_elim = TRUE)
wfs_result <-
  workflow()|>
  add_model(tune_lgb_eng)|>
  add_recipe(rcp_bs_v0)|>
  tune_race_anova(
    resamples = cv_folds,
    grid = tune_lgb_grid,
    metrics = rocauc_metrics,
    control=ctrl_race
  )
wfs_result|>show_best(metric='roc_auc')
  
plan(sequential)

```

### 8. Cross-Validation ----

```{r}
# combined it with step3 data splitting
```

### 9. Tuning and Evaluation ----

```{r}
# plan(multisession,workers =2)
# cars_tune_results <- cars_workflow |>
#   tune_grid(
#     resamples = cars_folds,
#     grid = cars_grid,
#     metrics = metric_set(rmse),
#      control = control_grid(save_pred = TRUE, 
#                             verbose = TRUE,
#                             allow_par = F) # Keep predictions
#   )
#  
#  # Find best parameters
#  best_params <- cars_tune_results |>
#    select_best("rmse")
# 
#  # Finalize workflow with best parameters
#  final_workflow <- cars_workflow |>
#    finalize_workflow(best_params)
```

```{r}
# Fit the final workflow to the training data
# final_lgbm_fit <- last_fit(final_workflow,cars_split )
# final_lgbm_mod <- extract_workflow(final_lgbm_fit )
# collect_metrics(final_lgmb_mod)

# plan(sequential)

```

### 10. Evaluate on Test Set ----

```{r}
combined_test_result <- 
  test_set %>%
  bind_cols(predict(combined_fit, .))
combined_test_result|>roc_auc(response, .pred_1)
```

### 11. Prepare Submission ----

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 12)
final_model <- simple_w
final_predictions <- final_model |>
   predict(new_data = test) 
plan(sequential)

 # #Handle negative predictions
 # final_predictions <- final_predictions |>
 #   mutate(.pred= ifelse(.pred< 0, 0, .pred))

 # Save submission file
 submission |>
   #mutate(response=final_predictions$.pred_class)|>
   readr::write_csv("submission.csv")
 zip('submission.csv.zip','submission.csv')
 
```

## kaggle submission

### notebook convert
```{r}
library(rmd2jupyter)
rmd2jupyter('ml_workflow.Rmd')
```

### score submit
```{r}
# # submit latest submission.csv
 system('kaggle competitions submit -c playground-series-s4e7 -f submission.csv.zip -m "sample_submission"')
 Sys.sleep(15)
# # get latest score 
 system('kaggle competitions submissions -q -c playground-series-s4e7')
# 
# # get leader board score
# #system('kaggle competitions leaderboard -s -v -c playground-series-s4e8')
```
